{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Scx0zwHEwDoH",
        "outputId": "e848a0bb-40e7-4d6d-869c-cbddde3742de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gitPython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitPython\n",
            "Successfully installed gitPython-3.1.31 gitdb-4.0.10 smmap-5.0.0\n",
            "Cloning into 'RLclass'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 40 (delta 8), reused 32 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), 7.45 KiB | 693.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# install gitPython\n",
        "import os, sys, time\n",
        "!pip install gitPython\n",
        "# clone my repository\n",
        "import git\n",
        "!git clone https://github.com/sungbinlim/RLclass.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/RLclass/STAT436/grid_world/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkrvAMRX4Uqh",
        "outputId": "f3b6e53b-4f1b-4868-e4ee-7accde31a301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RLclass/STAT436/grid_world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"/content/RLclass/STAT436/grid_world/\")\n",
        "from grid_world import *\n",
        "from dynamics import *"
      ],
      "metadata": {
        "id": "PLQsvgLx2cpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hacking Dynamics\n",
        "\n",
        "Bellman equation:\n",
        "\n",
        "$$\n",
        "\\mathbf{v}=\\mathbf{P}_{\\text{reward}}\\mathbf{r}+\\gamma\\mathbf{P}_{\\text{value}}\\mathbf{v}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{v}\\in\\mathbb{R}^{|\\mathcal{S}|}$, $\\mathbf{r}\\in\\mathbb{R}^{|\\mathcal{R}|}$, $\\mathbf{P}_{\\text{reward}}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{R}|}$, and $\\mathbf{P}_{\\text{value}}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{S}|}$ such that \n",
        "\n",
        "\\begin{aligned}\n",
        "\\left(\\mathbf{P}_{\\text{reward}}\\right)_{jq}=\\sum_{a_{p}\\in\\mathcal{A}}\\pi(a_{p}|s_{j})\\sum_{s_{i}\\in\\mathcal{S}}\\mathcal{P}(s_{i},r_{q}|s_{j},a_{p})=\\boldsymbol{\\pi}_{p}\\mathbf{P}_{i,j,p,q}\\mathbf{1}_{i}\\\\\n",
        "\\left(\\mathbf{P}_{\\text{value}}\\right)_{ji}=\\sum_{a_{p}\\in\\mathcal{A}}\\pi(a_{p}|s_{j})\\sum_{r_{q}}\\mathcal{P}(s_{i},r_{q}|s_{j},a_{p})=\\boldsymbol{\\pi}_{p}\\mathbf{P}_{i,j,p,q}\\mathbf{1}_{q}\n",
        "\\end{aligned}\n",
        "\n",
        "If $\\gamma \\in (0, 1)$ then we can solve the equation as follows\n",
        "$$\n",
        "\\mathbf{v}=(I-\\gamma\\mathbf{P}_{\\text{value}})^{-1}\\mathbf{P}_{\\text{reward}}\\mathbf{r}\n",
        "$$\n",
        "\n",
        "To compute action-value function $q_{\\pi}(s,a)$, we use the following formula:\n",
        "$$\n",
        "q_{\\pi}(s,a)=\\sum_{s'\\in\\mathcal{S},r\\in\\mathcal{R}} [r +\\gamma v_{\\pi}(s')]\\mathcal{P}(s',r|s,a)\n",
        "$$"
      ],
      "metadata": {
        "id": "KDjrRQup9G5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class pi_dynamics:\n",
        "    def __init__(self, pi, gamma, reward, dynamics):\n",
        "        self.pi = pi    \n",
        "        self.gamma = gamma\n",
        "        self.reward = reward\n",
        "        grid_world_dynamics = dynamics()\n",
        "        self.dynamics = grid_world_dynamics.dynamics\n",
        "        self.pi_dynamics = np.zeros_like(self.dynamics) # [current_state, next_state, action, value] \n",
        "        self.P_reward = np.zeros((12, 3))\n",
        "        self.P_value = np.zeros((12, 12))\n",
        "        self.pi_dynamics, self.P_reward, self.P_value = self.update_all()\n",
        "\n",
        "    def update_pi_dynamics(self, pi_dynamics):\n",
        "        \"\"\"\n",
        "        compute pi * dynamics\n",
        "        \"\"\"\n",
        "        for j in range(12):\n",
        "            for p in range(4):\n",
        "                # broadcasting\n",
        "                pi_dynamics[j, :, p, :] = self.pi[j, p] * self.dynamics[j, :, p, :]\n",
        "        return pi_dynamics\n",
        "\n",
        "    def compute_P_reward(self, P_reward):\n",
        "        \"\"\"\n",
        "        return P_reward[next_state, reward]: marginalize pi_dynamics in state\n",
        "        \"\"\"\n",
        "        for j in range(12):\n",
        "             for q in range(3):\n",
        "                 # marginalization\n",
        "                 P_reward[j, q] = np.sum(self.pi_dynamics[j, :, :, q])\n",
        "        return P_reward\n",
        "\n",
        "    def compute_P_value(self, P_value):\n",
        "        \"\"\"\n",
        "        return P_value[next_state, state]: marginalize pi_dynamics in reward\n",
        "        \"\"\"\n",
        "        # state -> state\n",
        "        for j in range(12):\n",
        "             for i in range(12):\n",
        "                 # marginalization\n",
        "                 P_value[j, i] = np.sum(self.pi_dynamics[j, i, :, :])\n",
        "        return P_value\n",
        "\n",
        "    def update_all(self):\n",
        "        return self.update_pi_dynamics(self.pi_dynamics), self.compute_P_reward(self.P_reward), self.compute_P_value(self.P_value)\n",
        "\n",
        "    def compute_state_value(self):\n",
        "        \"\"\"\n",
        "        return state-value function via closed-form formula\n",
        "        \"\"\"\n",
        "        coeff = np.eye(12) - self.gamma * self.P_value\n",
        "        inv_coeff = np.linalg.inv(coeff)\n",
        "        state_value = inv_coeff @ self.P_reward @ self.reward\n",
        "        return state_value\n",
        "\n",
        "    def compute_action_value(self):\n",
        "        \"\"\"\n",
        "        return action-value function using state-value function\n",
        "        \"\"\"\n",
        "        state_value = self.compute_state_value()\n",
        "        expectation_reward = np.zeros((12, 4))\n",
        "        expectation_value = np.zeros((12, 4))\n",
        "        for i in range(12):\n",
        "            for a in range(4):\n",
        "                expectation_reward[i, a] = self.reward @ np.sum(self.dynamics, axis=1)[i, a, :]\n",
        "                expectation_value[i, a] = self.gamma * state_value @ np.sum(self.dynamics, axis=3)[i, :, a]\n",
        "        action_value = expectation_reward + expectation_value\n",
        "        return action_value"
      ],
      "metadata": {
        "id": "ef69Psl3w6tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's run Grid World (using `randomAgent`)"
      ],
      "metadata": {
        "id": "6LfSWYeSJj0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "\n",
        "# policy function\n",
        "pi = np.array([0.25, 0.25, 0.25, 0.25]) #up, left, right, down\n",
        "pi = np.reshape(np.tile(pi, 12), (12, 4))\n",
        "# reward\n",
        "reward = np.array([1, 0 ,-1])\n",
        "\n",
        "# initialize dynamics with randomAgent\n",
        "init_dynamics = dynamics\n",
        "init_pi_dynamics = pi_dynamics(pi, gamma, reward, init_dynamics)\n",
        "state_value = init_pi_dynamics.compute_state_value()\n",
        "action_value = init_pi_dynamics.compute_action_value()"
      ],
      "metadata": {
        "id": "37sPgB_wJdYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run random action\n",
        "run_grid_world(pi, state_value, action_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZfiYzu4Jfdd",
        "outputId": "3922f700-fd14-45e8-8e9d-dba7ae9e4b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's run grid world!\n",
            "Success rate:28.000000000000004 %\n",
            "-----------------\n",
            "| ↑ | ↑ | ↑ | ↑ | \n",
            "-----------------\n",
            "| ↑ | z | ↑ | ↑ | \n",
            "-----------------\n",
            "| ↑ | ↑ | ↑ | ↑ | \n",
            "-----------------\n",
            "\n",
            "state value:\n",
            " [-0.999 -0.931 -0.898 -0.238 -1.113  0.    -1.623 -2.238 -1.25  -1.396\n",
            " -1.592 -1.825]\n",
            "action value:\n",
            " [[-0.982 -0.989 -0.922 -1.102]\n",
            " [-0.925 -0.989 -0.889 -0.922]\n",
            " [-0.827 -0.922 -0.235 -1.607]\n",
            " [-0.238 -0.238 -0.238 -0.238]\n",
            " [-1.011 -1.102 -1.102 -1.238]\n",
            " [ 0.     0.     0.     0.   ]\n",
            " [-1.093 -1.607 -2.215 -1.576]\n",
            " [-2.238 -2.238 -2.238 -2.238]\n",
            " [-1.144 -1.238 -1.382 -1.238]\n",
            " [-1.387 -1.238 -1.576 -1.382]\n",
            " [-1.604 -1.382 -1.807 -1.576]\n",
            " [-2.111 -1.576 -1.807 -1.807]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration with greedy policy\n",
        "\n",
        "Let $\\pi^{(0)}$ be an initial policy and set $q_{\\pi^{(n)}}(s,a)$ be the action-value function of policy $\\pi^{(n)}$ for $n=0,1,2,\\ldots$ such that \n",
        "\n",
        "$$\n",
        "\\pi^{(n+1)}(a|s):=\\mathbf{1}_{a=a_{n}(s)},\\quad a_{n}(s):=\\underset{a\\in\\mathcal{A}}{\\text{argmax }}q_{\\pi^{(n)}}(s,a)\n",
        "$$\n",
        "\n",
        "Then $q_{\\pi^{(n)}} \\to q_{\\ast}$ as $n\\to\\infty$. "
      ],
      "metadata": {
        "id": "VdvMWqgU9ByK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(scalar, dim):\n",
        "    \"\"\"\n",
        "    scalar -> one-hot vector \n",
        "    \"\"\"\n",
        "    vec = np.zeros(dim)\n",
        "    vec[scalar] = 1\n",
        "    return vec\n",
        "\n",
        "# update policy w/ greedy policy\n",
        "def update_policy(policy, action_value):\n",
        "    \"\"\"\n",
        "    return greedy policy based on action-value function\n",
        "    \"\"\"\n",
        "    greedy_policy = np.zeros_like(policy)\n",
        "\n",
        "    for state in range(12):\n",
        "        action = np.argmax(action_value[state, :])\n",
        "        action = one_hot(action, 4)\n",
        "        greedy_policy[state] = action\n",
        "    \n",
        "    return greedy_policy\n",
        "\n",
        "# policy iteration\n",
        "def policy_iteration(pi, gamma, reward, dynamics, eps=1e-8):\n",
        "    \"\"\"\n",
        "    input: init_pi\n",
        "    output: optimal_pi\n",
        "    \"\"\"\n",
        "\n",
        "    # initial setting\n",
        "    init_dynamics = dynamics\n",
        "    dynamics_old = pi_dynamics(pi, gamma, reward, init_dynamics)\n",
        "    state_value_old = dynamics_old.compute_state_value() \n",
        "    action_value_old = dynamics_old.compute_action_value()\n",
        "\n",
        "    advances = np.inf\n",
        "    n_it = 0\n",
        "\n",
        "    while advances > eps:\n",
        "\n",
        "        # policy improvement\n",
        "        pi_new = update_policy(pi, action_value_old)\n",
        "        dynamics_new = pi_dynamics(pi_new, gamma=gamma, reward=reward, dynamics=init_dynamics)\n",
        "        \n",
        "        # policy evaluation\n",
        "        state_value_new = dynamics_new.compute_state_value()\n",
        "        action_value_new = dynamics_new.compute_action_value()\n",
        "        advances = np.sum(np.abs(state_value_new - state_value_old))\n",
        "        n_it += 1\n",
        "\n",
        "        # save policy and value functions\n",
        "        pi = pi_new\n",
        "        state_value_old = state_value_new\n",
        "        action_value_old = action_value_new\n",
        "                \n",
        "    print(\"Policy iteration converged. (iteration={}, eps={})\\n\".format(n_it, np.sum(advances)))\n",
        "\n",
        "    return pi_new, state_value_new, action_value_new"
      ],
      "metadata": {
        "id": "XJ7rYLai85_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's solve Grid World!"
      ],
      "metadata": {
        "id": "f0aNUe8u887L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update policy via value function\n",
        "start_time = time.time()\n",
        "pi_new, state_value_new, action_value_new = policy_iteration(pi, \n",
        "                                                             gamma, \n",
        "                                                             reward, \n",
        "                                                             init_dynamics)\n",
        "computation_time = time.time() - start_time\n",
        "print(\"Wall-clock time for Policy Iteration: {} sec\\n\".format(np.round(computation_time, 4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW5dPNaq7OJP",
        "outputId": "7387f64d-d132-48b0-fc13-17c304bf5f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy iteration converged. (iteration=4, eps=0.0)\n",
            "\n",
            "Wall-clock time for Policy Iteration: 0.0326 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run updated policy\n",
        "run_grid_world(pi_new, state_value_new, action_value_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNxDirTz7Tbx",
        "outputId": "ba14601e-febc-47b6-e54a-1b0f0fa6ca46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's run grid world!\n",
            "Success rate:99.0 %\n",
            "-----------------\n",
            "| → | → | → | ↑ | \n",
            "-----------------\n",
            "| ↑ | z | ↑ | ↑ | \n",
            "-----------------\n",
            "| ↑ | → | ↑ | ← | \n",
            "-----------------\n",
            "\n",
            "state value:\n",
            " [15.244 15.398 15.554 15.711 15.054  0.    15.179 13.711 14.86  14.803\n",
            " 14.953 14.803]\n",
            "action value:\n",
            " [[15.107 15.092 15.244 14.904]\n",
            " [15.245 15.092 15.398 15.244]\n",
            " [15.399 15.244 15.554 15.027]\n",
            " [15.711 15.711 15.711 15.711]\n",
            " [15.054 14.904 14.904 14.711]\n",
            " [ 0.     0.     0.     0.   ]\n",
            " [15.179 15.027 13.574 14.803]\n",
            " [13.711 13.711 13.711 13.711]\n",
            " [14.86  14.711 14.655 14.711]\n",
            " [14.675 14.711 14.803 14.655]\n",
            " [14.953 14.655 14.655 14.803]\n",
            " [13.805 14.803 14.655 14.655]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration\n",
        "For each $n=0, 1, 2,...$, we set\n",
        "\n",
        "$$\n",
        "q^{(n+1)}(s,a) =\\mathcal{M}(q^{(n)})(s,a):= \\sum_{s',r}[r+\\gamma \\max_{a'\\in\\mathcal{A}}q^{(n)}(s',a')]\\mathcal{P}(s',r|s,a)\n",
        "$$\n",
        "Then by the Banach fixed point theorem, we get\n",
        "$$\n",
        "q^{\\star}(s,a)=\\lim_{n\\to\\infty}q^{(n)}(s,a),\\quad q^{\\star}(s,a)=\\mathcal{M}(q^{\\star})(s,a)\n",
        "$$\n",
        "We will implement `Value Iteration` with the following tensors\n",
        "$$\n",
        "\\mathbf{Q}_{\\text{reward}}(s,a) = \\sum_{s',r}r\\cdot\\mathcal{P}(s',r|s,a),\\quad \\mathbf{Q}_{\\text{value}}(s,a)=\\sum_{s',r}\\gamma\\max_{a'\\in\\mathcal{A}}q(s',a')\\cdot\\mathcal{P}(s',r|s,a)\n",
        "$$"
      ],
      "metadata": {
        "id": "R2TvFsEB4UNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_Q_reward(reward, dynamics, broadcast=False):\n",
        "    \"\"\"\n",
        "    return Q_reward[state, action]\n",
        "    \"\"\"\n",
        "    Q_reward = np.zeros((12, 4))\n",
        "    \n",
        "    if not broadcast:\n",
        "        for a in range(4):\n",
        "            for i in range(12):\n",
        "                Q_reward[i, a] = np.sum(dynamics[i, :, a, :] @ reward) # state, next_state, action, reward\n",
        "    else:\n",
        "        dynamics = dynamics.transpose((0, 2, 1, 3)) # state, action, next_state, reward\n",
        "        dynamics = dynamics.reshape(-1, 12, 3)\n",
        "        Q_reward = Q_reward.squeeze()\n",
        "        Q_reward = np.sum(dynamics @ reward, axis=1)\n",
        "        Q_reward = Q_reward.reshape((12, 4))\n",
        "\n",
        "    return Q_reward\n",
        "\n",
        "def compute_Q_value(action_value, dynamics, gamma, broadcast=False):\n",
        "    \"\"\"\n",
        "    return Q_value[state, action]\n",
        "    WARNING: very slow if you don't use broadcasting!\n",
        "    \"\"\"\n",
        "    Q_value = np.zeros((12, 4))\n",
        "\n",
        "    if not broadcast:\n",
        "        for i in range(12):\n",
        "                for a in range(4):\n",
        "                    Q_value[i, a] = gamma * np.max(action_value, 1) @ np.sum(dynamics, axis=3)[i, :, a]\n",
        "    else:\n",
        "        dynamics = np.sum(dynamics, axis=3)\n",
        "        dynamics = np.transpose(dynamics, (0, 2, 1)) # state, action, next_state)\n",
        "        dynamics = dynamics.reshape(-1, 12)\n",
        "        Q_value = Q_value.squeeze()\n",
        "        Q_value = gamma * dynamics @ np.max(action_value, 1)\n",
        "        Q_value = Q_value.reshape((12, 4))    \n",
        "        \n",
        "    return Q_value\n",
        "\n",
        "# value iteration\n",
        "def value_iteration(init_action_value, gamma, reward, dynamics, eps=1e-8, broadcast=False):\n",
        "    \"\"\"\n",
        "    input: init action-value\n",
        "    output: optimal action-value\n",
        "    \"\"\"\n",
        "\n",
        "    # initial setting\n",
        "    init_dynamics = dynamics() \n",
        "    init_dynamics = init_dynamics.dynamics\n",
        "    action_value = init_action_value # state, action\n",
        "    \n",
        "    Q_reward = compute_Q_reward(reward, init_dynamics, broadcast)\n",
        "\n",
        "    advances = np.inf\n",
        "    n_it = 0\n",
        "\n",
        "    while advances > eps or n_it <= 3:\n",
        "\n",
        "        Q_value = compute_Q_value(action_value, init_dynamics, gamma, broadcast)\n",
        "        new_action_value = Q_value + Q_reward\n",
        "        advances = np.sum(np.abs(new_action_value - action_value))\n",
        "        action_value = new_action_value\n",
        "         \n",
        "        n_it += 1\n",
        "\n",
        "    print(\"Value iteration converged. (iteration={}, eps={})\".format(n_it, np.sum(advances)))\n",
        "\n",
        "    return new_action_value"
      ],
      "metadata": {
        "id": "LLU92UUg_Fyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "init_action_value = action_value\n",
        "optimal_action_value = value_iteration(init_action_value, gamma, reward, init_dynamics, eps=1e-5)\n",
        "computation_time = time.time() - start_time\n",
        "print(\"Wall-clock time for Value Iteration without broadcasting: {} sec\\n\".format(np.round(computation_time, 4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTtC1DzE4rzx",
        "outputId": "cee1d6d4-86c6-409f-a6fd-4d391d48a35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value iteration converged. (iteration=1341, eps=9.995866980361257e-06)\n",
            "Wall-clock time for Value Iteration without broadcasting: 1.7266 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "init_action_value = action_value\n",
        "optimal_action_value = value_iteration(init_action_value, gamma, reward, init_dynamics, eps=1e-5, broadcast=True)\n",
        "computation_time = time.time() - start_time\n",
        "print(\"Wall-clock time for Value Iteration with broadcasting: {} sec\\n\".format(np.round(computation_time, 4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMTUo-08z8dT",
        "outputId": "30ecc4b6-5ec4-45ea-d7ed-e75360da61bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value iteration converged. (iteration=1341, eps=9.995866999901182e-06)\n",
            "Wall-clock time for Value Iteration with broadcasting: 0.056 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi = np.array([0.25, 0.25, 0.25, 0.25]) #up, left, right, down\n",
        "pi = np.reshape(np.tile(pi, 12), (12, 4))\n",
        "pi_optimal = update_policy(pi, optimal_action_value) # update policy \n",
        "optimal_state_value = np.max(optimal_action_value, axis=1) "
      ],
      "metadata": {
        "id": "wJjXkiBI4rrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_grid_world(pi_optimal, optimal_state_value, optimal_action_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnXK8y3d5Rce",
        "outputId": "35b9616b-8883-4c52-9241-c2d29a6d442c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's run grid world!\n",
            "Success rate:99.0 %\n",
            "-----------------\n",
            "| → | → | → | ↑ | \n",
            "-----------------\n",
            "| ↑ | z | ↑ | ↑ | \n",
            "-----------------\n",
            "| ↑ | → | ↑ | ← | \n",
            "-----------------\n",
            "\n",
            "state value:\n",
            " [15.244 15.398 15.554 15.711 15.054  0.    15.179 13.711 14.86  14.803\n",
            " 14.953 14.803]\n",
            "action value:\n",
            " [[15.107 15.092 15.244 14.904]\n",
            " [15.245 15.092 15.398 15.244]\n",
            " [15.399 15.244 15.554 15.027]\n",
            " [15.711 15.711 15.711 15.711]\n",
            " [15.054 14.904 14.904 14.711]\n",
            " [ 0.     0.     0.     0.   ]\n",
            " [15.179 15.027 13.574 14.803]\n",
            " [13.711 13.711 13.711 13.711]\n",
            " [14.86  14.711 14.655 14.711]\n",
            " [14.675 14.711 14.803 14.655]\n",
            " [14.953 14.655 14.655 14.803]\n",
            " [13.805 14.803 14.655 14.655]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bLgIcMw5Woe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}