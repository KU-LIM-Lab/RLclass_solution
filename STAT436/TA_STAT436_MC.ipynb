{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvWZ4zU1bOUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf9f3c2-bacf-4816-fb1c-bc1ef558e223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gitPython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitPython\n",
            "Successfully installed gitPython-3.1.31 gitdb-4.0.10 smmap-5.0.0\n",
            "Cloning into 'RLclass'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 59 (delta 15), reused 44 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (59/59), 10.74 KiB | 423.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# install gitPython\n",
        "import os, sys, time\n",
        "!pip install gitPython\n",
        "\n",
        "# clone my repository\n",
        "import git\n",
        "!git clone https://github.com/sungbinlim/RLclass.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# append package address\n",
        "%cd /content/RLclass/STAT436/grid_world/\n",
        "sys.path.append(\"/content/RLclass/STAT436/grid_world/\") \n",
        "from grid_world import *"
      ],
      "metadata": {
        "id": "EWcN2rWybc1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd44f821-e2ee-40b4-f73b-e48bd903baed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RLclass/STAT436/grid_world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte-Carlo Methods\n",
        "\n",
        "Now we will deal with real case that we have no idea how to access dynamics, i.e., we cannot solve the Bellman equation nor optimality equation. Hence it is not desirable to use the `pi_dynamics` class. How can we obtain the optimal policy $\\pi_{\\ast}$?\n",
        "\n",
        "As a matter of fact, *Policy Iteration* requires value functions $v_{\\pi}$ and $q_{\\pi}$, which are attainable without solving the Bellman equation. We will see how to get this."
      ],
      "metadata": {
        "id": "Zu-7uCL0b7vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_position(state):\n",
        "    # state(12) -> array(3, 4)\n",
        "    return np.array([state // 4, state % 4])\n",
        "\n",
        "def reverse_position(array):\n",
        "    # array(3, 4) -> state(12)\n",
        "    return array[0] * 4 + array[1]\n",
        "\n",
        "def create_value_memory(dim=(12, 4)):\n",
        "    value_memory = np.zeros(dim)\n",
        "    return value_memory\n",
        "\n",
        "def action_to_index(action):\n",
        "    # up, left, right, down\n",
        "    if action == 'up':\n",
        "        return 0\n",
        "    if action == 'left':\n",
        "        return 1\n",
        "    if action == 'right':\n",
        "        return 2\n",
        "    if action == 'down':\n",
        "        return 3\n",
        "    else:\n",
        "        raise ValueError('not proper action')\n",
        "\n",
        "def transform_trajectory_memory(trajectory):\n",
        "    # trajectory -> {(state, action)}\n",
        "    trajectory_ = [(reverse_position(pair[0]), action_to_index(pair[1])) for pair in trajectory]\n",
        "\n",
        "    return trajectory_\n",
        "\n",
        "def mc_eval(history, reward_stat, gamma, update=0.99):\n",
        "    \"\"\"\n",
        "    input: history\n",
        "    output: value estimation\n",
        "    \"\"\"\n",
        "    value_memory = create_value_memory()\n",
        "    \n",
        "    for trajectory, reward in zip(history, reward_stat):\n",
        "        \n",
        "        trajectory_ = transform_trajectory_memory(trajectory)\n",
        "        T = len(trajectory_) - 1\n",
        "        G = reward\n",
        "\n",
        "        tmp = np.zeros_like(value_memory)\n",
        "        tmp[trajectory_[T][0], trajectory_[T][1]] = G\n",
        "\n",
        "        for t in range(1, T+1, 1):\n",
        "            G = gamma * G\n",
        "            tmp[trajectory_[T-t][0], trajectory_[T-t][1]] = G\n",
        "        \n",
        "        value_memory = value_memory + update * (tmp - value_memory)\n",
        "\n",
        "    return value_memory\n",
        "\n",
        "# from Policy Iteration\n",
        "def one_hot(scalar, dim):\n",
        "    vec = np.zeros(dim)\n",
        "    vec[scalar] = 1\n",
        "    return vec\n",
        "\n",
        "def greedy_action(array, dim):\n",
        "    vec = np.zeros(dim)\n",
        "    array_size = array.shape[0]\n",
        "    for _ in array:\n",
        "        vec[_] = 1 / array_size\n",
        "\n",
        "    return vec\n",
        "\n",
        "def argmax(vec, tie=True):\n",
        "    if tie:\n",
        "        return np.where(vec == np.max(vec))[0]\n",
        "    else: # ordinary argmax\n",
        "        return np.argmax(vec)\n",
        "\n",
        "# update policy w/ greedy policy\n",
        "def update_policy(policy, action_value):\n",
        "\n",
        "    greedy_policy = np.zeros_like(policy)\n",
        "\n",
        "    for state in range(12):\n",
        "\n",
        "        action = argmax(action_value[state, :])\n",
        "        action = greedy_action(action, 4)\n",
        "        greedy_policy[state] = action\n",
        "\n",
        "    return greedy_policy\n",
        "\n",
        "# Policy Improvement w/ MC\n",
        "def mc_policy_iteration(pi_init, agent, gamma, eps=1e-8, play_num=100, epsilon=None):\n",
        "\n",
        "    # call policy eval\n",
        "    pi = pi_init\n",
        "    agent_ = agent(pi_init)\n",
        "    epsilon_init = epsilon\n",
        "    history, reward_stat, success_rate = agent_.play(play_num, stat=False)\n",
        "    print(\"Iteration: 0, Success rate:{} %\".format(success_rate * 100))\n",
        "    action_value = mc_eval(history, reward_stat, gamma)\n",
        "\n",
        "    advances = np.inf\n",
        "    n_it = 0\n",
        "\n",
        "    while advances > eps or n_it <= 2:\n",
        "        \n",
        "        # policy improvement\n",
        "        pi_new = update_policy(pi, action_value)\n",
        "\n",
        "        # policy evaluation\n",
        "        agent_ = agent(pi_new, epsilon)\n",
        "        history, reward_stat, success_rate = agent_.play(play_num, stat=False)\n",
        "        action_value_new = mc_eval(history, reward_stat, gamma)\n",
        "\n",
        "        # stop condition\n",
        "        advances = action_value_new - action_value\n",
        "        # advances = advances * (advances > 0)\n",
        "        advances = np.abs(action_value_new - action_value)\n",
        "        advances = np.sum(advances)\n",
        "\n",
        "        # save policy and update values\n",
        "        pi = pi_new\n",
        "        action_value = action_value_new\n",
        "        n_it += 1\n",
        "        epsilon = epsilon_init / n_it\n",
        "\n",
        "        if n_it % 10 == 0:\n",
        "            print(\"Iteration: {}, Success rate:{} %, Error: {}, eps: {}\".format(play_num * n_it, success_rate * 100, advances, epsilon))\n",
        "\n",
        "    print(\"Monte-Carlo Policy Iteration converged. (Iteration={}, Error={})\".format(play_num * n_it, advances))\n",
        "\n",
        "    return pi_new, action_value_new"
      ],
      "metadata": {
        "id": "kv_TYBjpbhyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "gamma = 0.99\n",
        "# random policy function\n",
        "pi = np.array([0.25, 0.25, 0.25, 0.25]) #up, left, right, down\n",
        "pi = np.reshape(np.tile(pi, 12), (12, 4))\n",
        "\n",
        "print(\"\\nUpdating Policy via Policy Iteration w/ Monte-Carlo\")\n",
        "start_time = time.time()\n",
        "pi_new, action_value_new = mc_policy_iteration(pi, Agent, gamma, play_num=100, epsilon=0.1)\n",
        "end_time = time.time()\n",
        "computation_time = end_time - start_time\n",
        "print(\"Wall-clock time for Policy Iteration: {} sec\\n\".format(np.round(computation_time, 4)))\n",
        "\n",
        "print(\"Let's run grid world!\")\n",
        "agent = Agent(pi_new)\n",
        "success_rate = agent.play(100, stat=True)\n",
        "agent.show_policy()\n",
        "print(\"action value:\\n {}\".format(np.round(action_value_new, 3)))\n",
        "print(\"Success rate:{} %\".format(success_rate * 100))\n"
      ],
      "metadata": {
        "id": "okmpvN1PfPEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4fd31d-4777-4187-d2ee-c4a5cb8e2121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updating Policy via Policy Iteration w/ Monte-Carlo\n",
            "Iteration: 0, Success rate:26.0 %\n",
            "Iteration: 1000, Success rate:100.0 %, Error: 0.9605959906929904, eps: 0.01\n",
            "Iteration: 2000, Success rate:90.0 %, Error: 2.3986406845221917, eps: 0.005\n",
            "Iteration: 3000, Success rate:100.0 %, Error: 0.01930702880806834, eps: 0.0033333333333333335\n",
            "Iteration: 4000, Success rate:100.0 %, Error: 1.920813525790919e-06, eps: 0.0025\n",
            "Monte-Carlo Policy Iteration converged. (Iteration=4400, Error=9.415733559358212e-53)\n",
            "Wall-clock time for Policy Iteration: 43.3109 sec\n",
            "\n",
            "Let's run grid world!\n",
            "-----------------\n",
            "| → | → | → | ↑ | \n",
            "-----------------\n",
            "| ↑ | z | ↑ | ↑ | \n",
            "-----------------\n",
            "| ↑ | ← | ↑ | ↑ | \n",
            "-----------------\n",
            "action value:\n",
            " [[0.    0.    0.98  0.   ]\n",
            " [0.    0.    0.99  0.   ]\n",
            " [0.    0.    1.    0.   ]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.97  0.    0.    0.   ]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.961 0.    0.    0.   ]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.   ]]\n",
            "Success rate:100.0 %\n"
          ]
        }
      ]
    }
  ]
}